{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a really useful tool for data analysis.  It can be used to lessen the storage/computation requirements for a dataset via dimensionality reduction.  This involves a coordinate transformation from the original feature space to a space defined by the principal components, which are linear recombinations of those features.  It can be quite powerful, reducing a data set from 1000s of features to a handful, which can reproduce ~99% of the variance in the sample.\n",
    "\n",
    "Another powerful use of PCA is for data mining and explanatory data models.  Using the principal components, we can understand the relationships in the data that describe the largest variations away from the mean behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will use is a sample of ~1000 randomly generated ice cream sundaes.  We have different flavors of Amy's Ice Creams, various amounts of toppings, and the corresponding calories per sundae.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the data.\n",
    "sundae_df = pd.read_csv('IceCreamSundaes.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sundae_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships in the data\n",
    "Before we get started, let's have a look at the data.  What relationships do you notice?  Take a moment on Slack to share the relationships you notice in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A scatter plot is a grid of every feature vs. every other feature.  Along the diagonal, where the feature would be\n",
    "# plotted against itself, a kernal density estimate is plotted, showing the distribution of values for that feature.\n",
    "\n",
    "scatter_matrix(sundae_df, alpha=0.2, figsize=(10, 10), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't, note the range of values for each feature.  When PCA looks for the main driver of variation in the data, what do you think it will find?  (I.e. what will be responsible for the biggest numerical difference from one sundae to the next?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prep data for PCA\n",
    "\n",
    "del sundae_df['flavor'] #remove text column\n",
    "X = sundae_df/np.std(sundae_df) #divide by stddev to normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the largest variations, PCA needs to know how the data behaves on average.  It calculates the means for each feature and subtracts them from the data under the hood, then proceeds to calculate the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Applying a PCA is really easy.\n",
    "\n",
    "pca = PCA() # Initialize the class.\n",
    "pca.fit(X) # Find the PCs.\n",
    "\n",
    "# Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Dimensions\n",
    "\n",
    "Ok, we've run a PCA.  How do we know how many principal components (PCs) to keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the amount of the variance in the data set represented in each PC.\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(6,4))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "\n",
    "PCnums = [x+1 for x in range(len(pca.explained_variance_ratio_))]\n",
    "sns.barplot(PCnums,np.cumsum(pca.explained_variance_ratio_),ax=ax1)\n",
    "\n",
    "ax1.set_title('Cumulative Sum of Variance Explained by PCs')\n",
    "ax1.set_xlabel('Number of PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many PCs do you think we should keep and why?\n",
    "Talk with your group about how many PCs you think are necessary.  Share on Slack when you decide.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enter the number of PCs you want to keep below as numPCs.\n",
    "numPCs = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Coordinates\n",
    "How do we take the PCs to reduce the size of our data set?\n",
    "\n",
    "First, let's take a look at the PCs themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The pca class has several attributes associated with it, which end in '_'.  \n",
    "# pca.components_ yields the contribution of each feature to each PC.\n",
    "# For PC1 (row 0), there is a -0.155670 contribution from ice_cream, etc.  \n",
    "# We will evaluate what these numbers mean shortly.  \n",
    "\n",
    "# For now, we can use them to transform our data and reduce our dimensions.\n",
    "\n",
    "pca_df = pd.DataFrame(pca.components_, columns = sundae_df.columns)\n",
    "pca_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's recreate one of our sundaes.  Say sundae #5.\n",
    "X.iloc[5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to know how much of each PC to add to the mean to recreate Sundae #5.  These are called the weights.\n",
    "weights = pca.transform(X)\n",
    "weights[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you know you're going to use PCA to transform your data, you can \n",
    "# run the PCA and the transformation all in one step:\n",
    "#weights = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = pd.DataFrame([pca.mean_],columns = sundae_df.columns)\n",
    "meanPCs = pd.concat([mean,pca_df[0:numPCs]])\n",
    "meanPCs = meanPCs.reset_index(drop=True)\n",
    "meanPCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize what sundae #5 looks like.\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot sundae #5\n",
    "X.iloc[5,:].plot(ax=ax,kind='bar',alpha=0.4)\n",
    "\n",
    "# Uncomment the next line and run this cell to additionally plot mean values for dataset\n",
    "#meanPCs.iloc[0].plot(ax=ax,kind='bar',alpha=0.2,color='g')\n",
    "\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Sundae #5')\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe sundae #5 on Slack.  Would you eat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize what sundae #5 looks like.\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot sundae #5\n",
    "X.iloc[5,:].plot(ax=ax,kind='bar',alpha=0.4)\n",
    "\n",
    "# Now we need to add weight*PC for each PC you want to include.  Try adding one PC at a time.\n",
    "# First, we'll start with just the mean + PC1.\n",
    "sundae5 = meanPCs.iloc[0] + weights[5][0]*meanPCs.iloc[1]\n",
    "\n",
    "sundae5.plot(ax=ax,kind='bar',alpha=0.2,color='orange')\n",
    "\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Sundae #5')\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and revise the previous plot.  Continue to add weights x PCs to your sundae #5 reconstruction and see how the plot evolves.  \n",
    "\n",
    "#### Does the reconstruction get closer to the data with additional PCs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember where we came from:\n",
    "You should notice that you come very close to recreating sundae #5 from our X data set that we inputted to the PCA.  Recall that we divided the original data, sundae_df, by the standard deviation.  So to truly recover the original data, you would need to multiply your reconstructed sundae #5 by the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting PCA\n",
    "\n",
    "Coming back to PCA as a tool for data mining and explanatory models, let's investigate our PCs in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's look again at our DF of the sample means + PCs\n",
    "meanPCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot just PC1\n",
    "figure = plt.figure(figsize=(12,6))\n",
    "ax = figure.add_subplot(111)\n",
    "\n",
    "#plot PC1\n",
    "meanPCs.iloc[1].plot(ax=ax,kind='bar',alpha=0.4,color='g')\n",
    "\n",
    "#add horizontal dashed line at 0\n",
    "xzero = range(-1,10)\n",
    "zero = [0 for xi in xzero]\n",
    "ax.plot(xzero,zero,'m--')\n",
    "\n",
    "ax.set_title('Principal Component 1')\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By evaluating how different features contribute to the PCs, we can say something about important relationships in our data.  Any features that have the same sign (positive or negative) are positively correlated.  Features with opposite signs are anticorrelated.  The value of each feature's contribution tells you how strong those relationships are.  PC1 will include the correlations that describe the most variation among your data.  \n",
    "\n",
    "For ice cream sundaes in this data set, for example, one of the most important relationships is a strong correlation between the amount of hot fudge and marshmallow fluff.\n",
    "\n",
    "What are the most important relationships that differentiate one sundae from another?  (I.e. ALL the relationships represented in PC1.)  Talk with your group then post your answer on Slack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,18))\n",
    "for i in range(numPCs):\n",
    "    ax = fig.add_subplot(numPCs,1,i+1)\n",
    "    meanPCs.iloc[i+1].T.plot(sharex=True,ax=ax,kind='bar',alpha=0.75)\n",
    "    zero = [0 for x in pca_df.columns]\n",
    "    xzero = range(len(zero))\n",
    "    ax.plot(xzero,zero,'k--')\n",
    "    ax.set_title('PC'+str(i+1))\n",
    "\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive modeling\n",
    "\n",
    "If we were to use this data set for predictive modeling, we would likely want to use the ingredient features to predict the calories.  Given that, can you glean any further information from the PC plots?  Would you revise the number of components you want to keep?  What else might you consider?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
